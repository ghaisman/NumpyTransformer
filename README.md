The goal of this project is to create a functional transformer model from scratch only using numpy. The structure deviates slightly from the classical transformer model described in "Attention is all you need" in that there is only one encoder layer, and instead of a layered decoder there is just a standard feedforward neural network. These architecture choices reflect the potential uses for this model which will be implemented later.
